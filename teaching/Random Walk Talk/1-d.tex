\documentclass[11pt]{article}


%%%%%%---------------------------------------------------------------------------------------------------
%%%%%%-----------------  Definitions of Wenjie Ye-----------------------------------------------------

\usepackage{mathrsfs,amssymb,amsmath,amsfonts}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{tipa}
\usepackage{enumerate}
\usepackage{color}
\usepackage[titletoc,title]{appendix}
\usepackage{mathtools}
\usepackage{cite}
\usepackage[hyphens]{url}
\usepackage[bookmarks=false,hidelinks]{hyperref}
%\usepackage{showkeys}

\usepackage{arydshln}% For a complax matrix



\topmargin=0pt \pagestyle{plain} \raggedbottom %\topmargin=-1cm 
\topmargin=-2.25cm 
\oddsidemargin=0mm \textwidth 160mm \textheight 240mm


%\renewcommand{\arraystretch}{1.8}



\newtheorem{assumption}{Assumption}%[section]

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}





%\newenvironment{proof}{{\it Proof.}}{~\hfill $\Box$}
\newenvironment{keywords}{{\bf Key words: }}{}
%\newenvironment{AMS}{{\bf AMS subject classification: }}{}


%\numberwithin{equation}{section}



\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\esssup}{\mathop{\mathrm{esssup}}}
\newcommand{\essinf}{\mathop{\mathrm{essinf}}}

\newcommand{\var}{\mathop{\mathrm{Var}}}
\DeclareMathOperator{\tr}{Tr}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
%\numberwithin{equation}{section}

\newcommand{\udots}{\mathinner{\mskip1mu\raise1pt\vbox{\kern7pt\hbox{.}}
\mskip2mu\raise4pt\hbox{.}\mskip2mu\raise7pt\hbox{.}\mskip1mu}}

%%%%%%---------------------------------------------------------------------------------------------------

\long\def\xia#1{{\color{red}\footnotesize Xia:\ #1}}
\long\def\ye#1{{\color{blue}\footnotesize Ye:\ #1}}



\begin{document}

\title{ \bf Random Walk in Random Environment Chapter $4$
}
\author{Wenjie Ye}
% \author{Xia Xu\footnote{Academy of Mathematics and Systems Science, Chinese Academy of Sciences, Beijing 100190, China.  email: {\tt xxia@amss.ac.cn(Xia.\,Xu)}. } \ and Wenjie Ye\footnote{School of Mathematics and Statistics, Fujian Normal University, Fuzhou 350117, China. email: {\tt yewenjiefjnu@gmail.com(W.\,Ye)}} }

\maketitle
% \begin{abstract}
% In this paper we will give...
% \end{abstract}

% \begin{keywords}

% \end{keywords}
% aaaa


% \ye{This is Ye's comments.}\\
% \noindent\xia{This is Xia's comments.}
\section{$G(x)=H(x)$}
We give the proof about $G(x)=H(x)$.
\begin{equation*}
  \begin{aligned}
   \int^x_{-x} \sum^{+\infty}_{k=-\infty} (-1)^k\exp\left( -\frac{1}{2}(u-2kx)^2\right)du   &= \sum^{+\infty}_{k=-\infty} (-1)^k \int^{(2k+1)x}_{(2k-1)x} e^{-\frac{1}{2} u^2}\,du\\ 
&=\int^{+\infty}_{-\infty} \sum_{k=-\infty}^{+\infty}(-1)^k \mathbf{1}_{[(2k-1)x, (2k+1)x]}(u) e^{-\frac{1}{2}u^2}\,du.
  \end{aligned}
\end{equation*}
It is obvious that $\sum_{k=-\infty}^{+\infty}(-1)^k \mathbf{1}_{((2k-1)x, (2k+1)x)}(u)$ is a $4x$-periodic function and is even(consider function graph).
\[
\sum_{k=-\infty}^{+\infty}(-1)^k \mathbf{1}_{((2k-1)x, (2k+1)x)}(u)=a_0+\sum_{n=1}^{+\infty} a_n \cos\left( \frac{2n\pi}{4x}u \right),
\]
where $$a_0=\frac{1}{4x} \int_{-2x}^{2x}  \left(-\mathbf{1}_{(-2x, -x)}(u) +\mathbf{1}_{(-x, x)}(u) -\mathbf{1}_{(x, 2x)}(u)  \right)\,du=0$$
and 
\begin{equation*}
  \begin{aligned}
     a_n& =\frac{2}{4x}\int^{2x}_{-2x}\left(-\mathbf{1}_{(-2x, -x)}(u) +\mathbf{1}_{(-x, x)}(u) -\mathbf{1}_{(x, 2x)}(u)    \right)\cdot \cos\left(\frac{2n\pi}{4x} u\right)\,du\\
&=\frac{4}{n\pi}  \sin\left(\frac{1}{2}n\pi\right). 
  \end{aligned}
\end{equation*}
Therefore, 
\begin{equation*}
  \begin{aligned}
   \sum_{k=-\infty}^{+\infty}(-1)^k \mathbf{1}_{((2k-1)x, (2k+1)x)}(u)&=\sum_{n=1}^{+\infty} \frac{4}{n\pi}  \sin\left(\frac{1}{2}n\pi\right)\cos\left( \frac{2n\pi}{4x}u \right)\\ 
&=\frac{4}{\pi}\sum^{+\infty}_{k=0} \frac{1}{n}\sin\left( \frac{1}{2}(2k+1)\pi \right)\cos\left( \frac{(2k+1)\pi}{2x} u\right)\\ 
&=\frac{4}{\pi} \sum^{\infty}_{k=0} \frac{(-1)^k}{2k+1}\cos\left( \frac{(2k+1)\pi}{2x} u\right).
  \end{aligned}
\end{equation*}
Now, we have
\begin{equation*}
  \begin{aligned}
     & \frac{1}{\sqrt{2\pi}}\int^x_{-x} \sum^{+\infty}_{k=-\infty} (-1)^k\exp\left( -\frac{1}{2}(u-2kx)^2\right)du  \\ 
=&\int^{+\infty}_{-\infty} \frac{4}{\pi} \sum^{\infty}_{k=0} \frac{(-1)^k}{2k+1}\cos\left( \frac{(2k+1)\pi}{2x} u\right) \cdot\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2} u^2}\,du\\ 
=&\mathbb{E}\left[ \frac{4}{\pi} \sum^{\infty}_{k=0} \frac{(-1)^k}{2k+1}\cos\left( \frac{(2k+1)\pi}{2x} \mathbf{X}\right)\right]\\
=&\frac{4}{\pi} \sum^{\infty}_{k=0} \frac{(-1)^k}{2k+1}\mathbb{E}\left[ \cos\left( \frac{(2k+1)\pi}{2x} \mathbf{X}\right)\right]
  \end{aligned}
\end{equation*}
where $\mathbf{X}\sim N(0,1)$.
Note that 
\begin{equation*}
  \begin{aligned}
   &\mathbb{E}\left[\cos\left( \frac{(2k+1)\pi}{2x} \mathbf{X}\right)+i\sin\left( \frac{(2k+1)\pi}{2x} \mathbf{X}\right) \right]\\  
& =\mathbb{E}\left[ \exp\left(i \frac{(2k+1)\pi}{2x} \mathbf{X} \right) \right]= \exp\left(- \frac{(2k+1).\pi^2}{8x^2} \right)
  \end{aligned}
\end{equation*}
We proved that $G(x)=H(x)$.

\section{ Recurrence of  random walk on $\mathbb{Z}$ by optional stopping}
\begin{theorem}[Optional stopping theorem]
Let $\{M_n\}_{n\in \mathbb{N}}$ be a martingale and $T$ is a stopping time respect to filtration $\{\mathscr{F}_n\}_{n\in \mathbb{N}}$. We have  that $\mathbb{E}[\abs{M_T}]<\infty$ and $\mathbb{E}{M_T}=\mathbb{E}[M_0]$ if one of the following holds
\begin{enumerate}[(i)]
  \item The stopping time $T$ is a.s. bounded; that is, there exists $C \ge 0$ such that $T \le C$ a.s..
  \item $T<\infty$ a.s. and $\{M_n\}_{n\in \mathbb{N}}$ is uniformly integrable and $\mathbb{E}[\abs{M_T}]<\infty$.
\end{enumerate}
\end{theorem}
\begin{remark}
  Note that the condition $\mathbb{E}[\abs{M_T}]<\infty$ is redundant. Indeed, by the martingale convergence theorem we know that if $\{M_n\}_{n\in \mathbb{N}}$ is a martingale with $\sup_n \mathbb{E}[\abs{M_n}]<\infty$, then there exists a random variable $M_\infty$ such that $M_n\rightarrow M_\infty \ a.s.$ and $\mathbb{E}[\abs{M_\infty}]<\infty$. Let $\mathbf M_n=M_{T\wedge n}$, then we have
\[
\sup_n \mathbb{E}[\abs{\mathbf M_n}]\le \sup_n \mathbb{E}[\abs{M_{T\wedge n}}]{\color{red}\le \sup_n \mathbb{E}[\abs{M_n}]<\infty}.
\]
{\color{blue} In fact, $\mathbb{E}[M_{T\wedge n}]\le \mathbb{E}[M_n]$ for $T<\infty \ a.s. $. It is obvious that $\{\abs{M_n}\}_{n\in \mathbb{N}}$ is a sub-martingale, define $U_n=\abs{M_n}-\abs{M_{T\wedge n}}$, we can obtain that 
\[
U_{n+1}-U_n=(\abs{M_{n+1}}-\abs{M_n})\mathbf{1}_{\{T\le n\}}
\]
due to $\abs{M_{T\wedge(n+1)}} - \abs{M_{T\wedge n}}=(M_{n+1}-M_n) \mathbf{1}_{\{ T>n\}} $. Therefore,
\[
\mathbb{E}[U_{n+1}-U_n]=\mathbf{1}_{\{ T\le n \}}\cdot \mathbb{E}[\abs{M_{n+1}}-\abs{M_n}|\mathscr{F}_n]\ge 0.
\]
}
It is obvious that  $\{\mathbf M_n\}_n=\{M_{T\wedge n}\}_n$ is  martingale, therefore, we have
\[
\mathbf M_n\rightarrow \mathbf M_\infty\ a.s..
\]
and $\mathbb{E}[\abs{\mathbf M_\infty}]<\infty$(i.e. $\mathbb{E}[M_T]<\infty$ since $T<\infty \ a.s. $).
\end{remark}

\begin{remark}
  The Optional stopping theorem is important, Let $T=\inf\{n: S_n=1  \}$, we can prove that $T<\infty\  a.s. $(recurrent), but $S_T=1\ a.s. $, $S_0=0$, we observe that $\mathbb{E}[S_T]\neq \mathbb{E}[S_0]$.
\end{remark}
It is obviously that $\{S_n\}_{n\in \mathbb{N}_+}$ is a martingale. Let $T_z:= \inf\{ n:S_n=z \}$, then $T_z$ is a stopping time. For $a<0<b$, define stop time $T_{a,b}:=T_a\wedge T_b$, which is the first exit time of $(a,b)$. Since $M_n:= S_{T_{a,b}\wedge n} \le \abs{a}\vee \abs{b}\ a.s.$, and {\color{red} $T_{a,b}<\infty\ \mathbb{P}_0\text{-}a.s.$}, we have
\begin{equation*}
  \begin{aligned}
     & 0=\mathbb{E}[M_{T_{a,b}}]=\mathbb{P}(T_a<T_b)\cdot a + \mathbb{P}(T_b<T_a)\cdot b =\mathbb{P}(T_a<T_b)(a-b) +b.
  \end{aligned}
\end{equation*}
{\color{red} (where $T_{a,b}<\infty \ a.s.$ is due to that Wald's identities and Dominated convergence theorem)} can obtain $\mathbb{E}[T_{a,b}]<\infty$.  Therefore, 
\[
\mathbb{P}_0(T_a<T_b)=\frac{b}{b-a}.
\]
Note that $i+S_n$  have the distribution of a random walk started at $i$, Thus, for all $0\le i\le k$, 
\[
\mathbb{P}_i(T_0<T_k)=\mathbb{P}_0(T_{-i}<T_{k-i})=\frac{k-i}{k}.
\]
Note that 
\[
\mathbb{P}_i(T_0=\infty)=\lim_{n\rightarrow \infty}\mathbb{P}_i(T_n<T_0)=0,
\]
\begin{remark}
  We also can prove that $T_{a,b}<\infty\ a.s.$ by estimating $\mathbb{P}(T_{a,b}>n I)$ where $I:=b-a$.
In fact, for any $a<x<b$,
\[
\mathbb{P}(S_{n+I}\notin (a,b) |S_n=x,T_{a,b}>n)\ge \mathbb{P}(\forall \ 0\le j<I, X_{n+j+1}=1| S_n=x,T_{a,b}>n)=2^{-I}.
\]
Since $T_{a,b}>n+I$ implies that $T_{a,b}>n$ and $S_{n+I}\in (a,b)$, therefore, we have 
\begin{equation*}
  \begin{aligned}
     & \mathbb{P}(T_{a,b}>n+I)\\
=&\sum^{b-1}_{x=a+1} \mathbb{P}(T_{a,b}>n+I| S_n=x, T_{a,b}>t)\cdot \mathbb{P}(S_n=x,T_{a,b}>n)\\
\le& (1-2^{-I})\cdot \mathbb{P}(T_{a,b}>n)
  \end{aligned}
\end{equation*}
Inductively, we have
\[
\mathbb{P}(T_{a,b}>nI)\le (1-2^{-I})^n.
\]
\end{remark}
\section{Borel-Cantelli Lemma and almost sure convenience}

{\bf The proofs of almost all strong theorem are based on different forms of the Borel-Cantelli Lemma and those of the Markov inequality.} {\em The main idea of Borel-Cantelli Lemma is to construct a series to control the probability of evens.}
\begin{lemma}\label{1}
  If $\sum_{n=1}^\infty \mathbb{P}(A_n)<\infty$, then $\mathbb{P}(\limsup_{n\rightarrow \infty} A_n):=\mathbb{P}(A_n\ i.o.)=0$.
\end{lemma}
\begin{proof}
  Define general r.v. $\xi:=\sum^\infty_{n=1} \mathbf{1}_{A_n}$, it is obvious $\xi$ is not negative. By $\mathbb{E}[\xi]=\sum^\infty_{n=1}\mathbb{P}(A_n)<\infty$, we have $\xi<\infty$ a.s., which is due to 
\[
\mathbb{P}(\xi=\infty)\le \mathbb{P}(\xi\ge N)\le \frac{1}{N}\mathbb{E}[\xi].
\]
By $\xi<\infty$ a.s., we have $\mathbb{P}(A_n\ i.o.)=0$.
\end{proof}
\begin{proof}
\[
  \mathbb{P}(\limsup_{n\rightarrow \infty} A_n)=\lim_{n\rightarrow \infty}\mathbb{P}(\cup_{k\ge n} A_k)\le \lim_{n\rightarrow \infty} \sum^\infty_{k=n} \mathbb{P}(A_k)=0.
\]
\end{proof}
\begin{corollary} If
  \begin{enumerate}[(i)]
    \item $\sum_{n=1}^\infty \mathbb{P}({A_n}|B_n)<\infty$,
    \item $B_n$ occurs a.s. if $n$ is large enough,
  \end{enumerate}
then ${A}_n$ occurs a.s. only finitely many times.
\end{corollary}
\begin{proof}
  By Lemma \ref{1}, we have 
\[
\sum^\infty_{n=1}\mathbb{P}(A_n\cap B_n)\le \sum^\infty_{n=1}\mathbb{P}(A_n| B_n)\mathbb{P}(B_n)\le \sum^\infty_{n=1}\mathbb{P}(A_n| B_n).
\]
Therefore, $A_nB_n$ occurs a.s. finitely many times. By $(ii)$, we complete the proof since $B_n=\Omega$ if $n$ is large enough.
\end{proof}

The converse of the Borel-Cantelli lemma is trivially false.
\begin{example}
  Let $\Omega=(0,1)$, $\mathcal{F}=\mathscr{B}({(0,1)})$ and $\mathbb{P}=\lambda$. If $A_n=(0,a_n)$ where $a_n\rightarrow 0$ as $n\rightarrow \infty$, then $\limsup A_n=\emptyset$, but if $a_n=\frac{1}{n}$, we have $\sum a_n=\infty$.
\end{example}


{\color{blue} \begin{lemma}\label{3}
  Let $S_n:=\sum^n_{k=1} X_k$, where $X_k\ge 0$. If $\mathbb{E}[S_n]\rightarrow \infty$, $\sup_{n\ge 1} \mathbb{E}[X_n]<\infty$ and we can find $C,\delta>0$ such that for any $n\in \mathbb{N}_+$,
\begin{equation}\label{2}
  \begin{aligned}
     & \mathbf{Var}(S_n)\le C\cdot\left(\mathbb{E}[S_n]\right)^{2-\delta}
  \end{aligned}
\end{equation}
then 
\[
\lim_{n\rightarrow \infty} \frac{S_n}{\mathbb{E}[S_n]}=1\ a.s..
\]
\end{lemma}
\begin{proof}
  We can assume $0<M:=\sup_{n\ge 1} \mathbb{E}[X_n]\le 1$. Note that $0\le \mathbb{E}[X_n]\le 1$ and $\mathbb{E}[S_n]\rightarrow \infty$, it is easy to see the integer part of $\{ E(n):=\mathbb{E}[S_n] \}_{n\ge 1}$ can  take all natural numbers. Therefore, we can find a subsequence $\{ n_k \}_{k\ge 1}$, such that
\[
k^{\frac{2}{\delta}}\le E(n_k)\le k^{\frac{2}{\delta}}+1,\quad \forall\,k \ge 1.
\]
By Markov's inequality, and \eqref{2}, we have
\[\mathbb{P}\left( \abs{\frac{S_{n_k}}{E(n_k)}-1}\ge \varepsilon \right)\le \frac{\mathbf{Var}(S_{n_k})}{\varepsilon^2\cdot E(n_k)^2}\le \frac{C}{\varepsilon^2\cdot k^2},\quad \forall k\ge 1, \varepsilon >0.
\]
By Borel-Cantelli's lemma, we have
\[
\lim_{k\rightarrow \infty}\frac{S_{n_k}}{E(n_k)}=1\ a.s..
\]
For $n$ large enough, there exista $k$ large enough such that $n\in [n_k,n_{k+1})$. In this time, utilize the monotonicity of $S_n$ and $E(n)$, we have
\begin{equation*}
  \begin{aligned}
     & \frac{E(n_k)}{E(n_{k+1})}\cdot \frac{S_{n_k}}{E(n_k)}\le \frac{S_n}{E(n)}\le \frac{E(n_{k+1})}{E(n_k)}\cdot \frac{S_{n_{k+1}}}{E(n_{k+1})}.
  \end{aligned}
\end{equation*}
Since $\frac{E(n_{k+1})}{E(n_k)}\rightarrow 1$ when $k\rightarrow \infty$, we complete the proof.
\end{proof}
}


\begin{lemma}
  If $\{A_n\}_{n\ge 1}$ are independent evens, then 
\[
\sum^\infty_{n=1} \mathbb{P}(A_n)=\infty \Rightarrow \mathbb{P}(A_n\ i.o.)=1.
\]
\end{lemma}
\begin{proof}
  Since $\mathbb{P}(\liminf_{n\rightarrow \infty} A_n)=\lim_{n\rightarrow} \mathbb{P}(\cup_{k\ge n} A_k^c)$, by the independence of $\{A_n\}_{n\ge 1}$, we have
\[
\mathbb{P}(\cap^m_{k=n} A_k^c)=\Pi^m_{k=n}\mathbb{P}(A^c_k)=(1-\mathbb{P}(A_k))\le \Pi^m_{k=n}\exp(-\mathbb{P}(A_k))=\exp\left(-\sum^m_{k=n}\mathbb{P}(A_k)\right)\rightarrow 0(m\rightarrow \infty).
\]
Therefore, 
\[
\mathbb{P}(\liminf_n A^c_n)=\lim_{n\rightarrow \infty}\mathbb{P}(\cap_{k=n}^\infty A^c_k)=\lim_{n\rightarrow \infty}\lim_{m\rightarrow \infty} \mathbb{P}(\cap^m_{k=n})=0.
\]
\end{proof}


In the following, we use  Corollary \ref{3} to prove
\begin{lemma}
    If $\{A_n\}_{n\ge 1}$ are  pairwise independent evens, then 
\[
\sum^\infty_{n=1} \mathbb{P}(A_n)=\infty \Rightarrow \mathbb{P}(A_n\ i.o.)=1.
\]
\end{lemma}
{\color{blue}\begin{proof}
Let $S_n:= \sum^{n}_{k=1} \mathbf{1}_{A_k}$, we compute the variation of $S_n$, for any $n\in \mathbb{N}_+$, 
\begin{equation*}
  \begin{aligned}
    \mathbf{Var}(S_n)  &=\sum^n_{k=1} \mathbf{Var}(\mathbf{1}_{A_k}) +2\sum_{1\le i< j\le n} \mathbf{Cov}(\mathbf{1}_{A_i},\mathbf{1}_{A_j})\\
&= \sum^n_{k=1}\mathbb{P}(A_k)- \sum^n_{k=1}\mathbb{P}^2(A_k)\le \mathbb{E}[S_n].
  \end{aligned}
\end{equation*}
Due to $\mathbb{E}[S_n]\rightarrow \infty$, by Corollary \ref{3}, we have $S_n\rightarrow \infty$ a.s..
\end{proof}}
\begin{proof}
  We only need to prove $\mathbb{P}(S_\infty\le a)=0\ \forall\,a>0$.  For any $a>0$, take $N\ge 1$ large enough, such that $\mathbb{E}[S_N]\ge a$. Then for any $n\ge N$, we have
\begin{equation*}
  \begin{aligned}
  \mathbb{P}(S_\infty\le a) &\le \mathbb{P}(S_n\le a)\\ 
&\le \mathbb{P}(-(S_n-\mathbb{E}[S_n])\ge \mathbb{E}[S_n]-a)\\ 
&\le \frac{\mathbb{E}[
\abs{S_n-\mathbb{E}[S_n]}^2]}{\abs{\mathbb{E}[S_n]-a}^2}=\frac{\mathbf{Var}(S_n)}{\abs{\mathbb{E}[S_n]-a}^2}\\ 
&\le \frac{\mathbb{E}[S_n]}{\abs{\mathbb{E}[S_n]-a}^2}\rightarrow 0,
  \end{aligned}
\end{equation*}
when $n\rightarrow \infty$, since $\mathbb{E}[S_n]\rightarrow \infty$.
\end{proof}

\begin{lemma}
  Let $A_1,A_2,cdots$ be a sequence  of evens for which
\begin{equation*}
  \begin{aligned}
     & \sum^\infty_{n=1} \mathbb{P}(A_n)=\infty, 
  \end{aligned}
\end{equation*}
and 
\begin{equation*}
  \begin{aligned}
     & \liminf_{n\rightarrow\infty}\frac{\sum^n_{k=1}\sum^n_{i=1} \mathbb{P}(A_kA_i)}{\left(\sum^n_{k=1}\mathbb{P}(A_k)\right)^2}\le C\quad(C\ge)
  \end{aligned}
\end{equation*}
then
\begin{equation*}
  \begin{aligned}
     & \mathbb{P}(\limsup_{n\rightarrow A_n})\ge C^{-1}.
  \end{aligned}
\end{equation*}
\end{lemma}






{\color{blue}The ideas to prove a.s. convergence by Borel-Cantelli lemma: 
\begin{lemma}
  Let $\{T_n\}_{n\ge  1}$ be r.v. such that
\begin{equation*}
  \begin{aligned}
     & \sum^\infty_{n=1} \mathbb{P}(\abs{T_n}>\varepsilon)<\infty
  \end{aligned}
\end{equation*}
for each $\varepsilon>0$. Then $T_n\rightarrow 0$ a.s..
\end{lemma}
\begin{proof}
  For each $k\ge 1$, 
\[
\sum_{n=1}^\infty \mathbb{P}(\abs{T_n}>2^{-k})<\infty.
\]
Hence, by the Borel-Cantelli lemma(use $[\limsup_n A_n ]^c)$, for each $k\ge 1$, $\abs{T_n}\le 2^{-k}$ for all $n$ sufficiently large, except on a null event $N_k$. It follows that
\begin{equation*}
  \begin{aligned}
     & T_n(\omega)\rightarrow 0\text{\quad for all $\omega \notin \cup^\infty_{k=1}N_k$}.
  \end{aligned}
\end{equation*}
Since $\cup^\infty_{k=1}N_k$ is a null event, $T_n\rightarrow 0$ a.s. follows.
\end{proof}

\begin{lemma}
  Let $\{T_n\}_{n\ge  1}$ be r.v. such that
\begin{equation*}
  \begin{aligned}
     & \sum^\infty_{n=1} \mathbb{P}(\abs{T_n}>\varepsilon_n)<\infty
  \end{aligned}
\end{equation*}
for positive constant $\varepsilon_n\rightarrow 0$. Then $T_n\rightarrow 0$ a.s..
\end{lemma}
\begin{proof}
  Applying Borel-Cantelli lemma to events $\{ \abs{T_n}>\varepsilon_n \}, n\ge 1$.
\end{proof}

We need to estimate $\mathbb{P}(\abs{T_n}>\varepsilon)$, it just the Chebyshev inequality
\[
\mathbb{P}(\abs{X}>\varepsilon)\le \mathbb{E}[\abs{X}]/\varepsilon
\]
\[
\mathbb{P}(\abs{X-\mathbb{E}[X]}>\varepsilon)\le \mathbf{Var}(X)/\varepsilon^2
\]
and 
\[
\mathbb{P}(X>\varepsilon)\le \exp(-t\varepsilon)\mathbb{E}[\exp(tX)]
\]
for each $\varepsilon >0$ and real $t$. 



The ideas to prove a.s. convergence. The moment estimate also is useful. 
\begin{lemma}\label{1.8}
  Suppose that 
\[
\sum^\infty_{n=1}\mathbb{E}[\abs{T_n}^p]<\infty,
\]
for some $p>0$, then $T_n\rightarrow 0$ a.s..
\end{lemma}
\begin{proof}
  By $\sum^\infty_{n=1}\mathbb{E}[\abs{T_n}^p]<\infty$, we have $\mathbb{E}\left[ \sum^\infty_{n=1} \abs{T_n}^p\right]<\infty$.  Moreover, $\sum^\infty_{n=1}\abs{T_n}^p<\infty$ a.s. and hence that $T_n\rightarrow 0$ a.s..

\end{proof}
The ideas to prove a.s. convergence by extracting subsequence.
\begin{lemma}
  Let $\{ X_n \}_{n \ge 1}$ is r.v. sequence, if there exists a subsequence $\{ n_k \}_{k\ge 1}$ such that
\[
X_{n_k}\rightarrow X\ a.s..
\]
and 
\[
\max_{n_{k-1}<n\le n_k} \abs{X_n-X_{n_{k-1}}}\rightarrow 0\ a.s..
\]
Then,
\[
X_n\rightarrow X\ a.s..
\]

\end{lemma}
\begin{proof}
  For $n$ large enough, there exist  a unique $k$ such $n_{k-1}<n\le n_k$, then
\[
\abs{X_n-X}\le \abs{X_{n_{k-1}} -X} +\max_{n_{k-1}<m\le n_k} \abs{X_m-X_{n_{k-1}}}\rightarrow 0\ a.s..
\]
\end{proof}



}

\begin{theorem}
\begin{equation}\label{5}
  \begin{aligned}
     & \lim_{n\rightarrow \infty} \frac{S_n}{n}=0 \quad a.s..
  \end{aligned}
\end{equation}
\end{theorem}
\begin{proof}
  Since $\mathbb{E}[\frac{S_n}{n}]=0$ and $\mathbb{E}[\frac{S_n^2}{n^2}]=\frac{1}{n}$, by Chebyshev inequality, for any $\varepsilon>0$, we have
\[
\mathbb{P}\left( \abs{\frac{S_n}{n}}\ge \varepsilon \right)\le \frac{1}{n\varepsilon^2},
\]
Therefore, we have $\frac{S_{n^2}}{n^2}\rightarrow 0$ a.s. when $n\rightarrow \infty$.  Now we have to  estimate the value of $S_k$ for the $k$ lying in the gap. If $n^2\le k <(n+1)^2$, then 
\begin{equation*}
  \begin{aligned}
     \abs{\frac{S_k}{k}} &=\abs{\frac{S_{n^2} n^2 }{kn^2}   +\frac{S_k-S_{n^2}}{k}}\\ 
&\le \abs{\frac{S_{n^2}}{n^2}} + \abs{\frac{k-n^2}{k}}\le \abs{\frac{S_{n^2}}{n^2}} + \abs{\frac{(n+1)^2-n^2}{k}} \rightarrow 0 \quad a.s..
  \end{aligned}
\end{equation*}
\end{proof}


\begin{proof}
Using $f(t):=\mathbb{E}[e^{tS_n}]=\left(\frac{e^t+e^{-t}}{2}\right)^n$, $\mathbb{E}[S_n^4]=f^{(4)}(t)|_{t=0}$. We have
$$\mathbb{E}\left[\frac{S_n^4}{n^4}\right]=n^{-3}+6 C^2_n n^{-4}=O(n^{-2}).$$
By Theorem \ref{1.8}, we complete the proof.
\end{proof}
\begin{remark}
  By Bernstein inequality, 
\[
\mathbb{P}\left( \abs{\frac{S_n}{n}}\ge \varepsilon \right)\le 2 \exp\left( -\frac{2n\varepsilon^2}{(1+2\varepsilon)^2} \right),
\]
it is obvious that  \eqref{5} holds true.
\end{remark}



\section{Between LLN and LIL}
By \eqref{5}, we have $\abs{S_n}=o(n) \ a.s.$, it is natural to ask whether a better rate can obtained, in fact we have.
\begin{theorem}
  For any $\varepsilon>0$,
\begin{equation*}
  \begin{aligned}
     & \lim_{n\rightarrow \infty} \frac{S_n}{n^{\frac{1}{2}+\varepsilon}}= 0 \ a.s..
  \end{aligned}
\end{equation*}
\end{theorem}
\begin{proof}
  For any a position integer, by $\mathbb{E}[S_n^{2K}]=f^{(2K)}(t)|_{t=0}$, we have
\[
\mathbb{E}[S_n^{2K}]=O(n^K).
\]
Note that for $2\varepsilon K>1$, we have
\[
\mathbb{E}\left[ \abs{\frac{S_n^{2K}}{n^{K+2\varepsilon K}}} \right]\lesssim \frac{1}{n^{2\varepsilon K}}. 
\]
By Lemma \ref{1.8}, we complete the proof.
\end{proof}

{\color{blue} By Borel-Cantelli lemma, we can obtain 
\begin{theorem}
\[
\limsup_{n\rightarrow \infty} \frac{\abs{S_n}}{n^{\frac{1}{2}}\log n}\le 1\ a.s..
\]
\end{theorem}
\begin{proof}
  By $\mathbb{E}[e^{tS_n}]=\left(\frac{e^t+e^{-t}}{2}\right)^n$, we have 
\[
\mathbb{E}\left[\exp\left(n^{-\frac{1}{2}}S_n \right)\right]\rightarrow e^{1/2}.
\]
Hence, 
\[
\mathbb{P}(S_n\ge (1+\varepsilon)n^{\frac{1}{2}}\log n)=\mathbb{P}\left(\exp(n^{-\frac{1}{2}}S_n)\ge n^{1+\varepsilon}\right)\lesssim \frac{1}{n^{1+\varepsilon}}
\]
Moreover, 
\[
\limsup_{n\rightarrow \infty}\frac{S_n}{n^\frac{1}{2} \log n}\le 1\ a.s..
\]
By the symmetry of $S_n$ i.e. $S_n$ equal  to $-S_n$  in law, we complete the proof.
\end{proof}
}
\begin{theorem} For any $\varepsilon>0$,
 \[ \lim_{n\rightarrow \infty}\frac{\mathbf{S}_n}{\sqrt{n(\log n)^{1+\varepsilon}}}=0\ a.s..
\]
\end{theorem}
\begin{proof}
First, we prove Kolmogorov's maximal inequality. Let 
 $X_1, X_2,\cdots$ be independent, mean-zero and $\mathbb{E}[X_k^2]<\infty\ \forall k\in \mathbb{N}_+$. Then
\[
\mathbb{P}\left( \sup_{1\le k\le n} \abs{S_k}>\lambda \right)\le \frac{\mathbb{E}[S^2_n]}{\lambda^2}=\frac{1}{\lambda^2}\sum^n_{k=1} \mathbf{Var}(X_k).
\]
We partition $A^*:=\{ \sup_{1\le k\le n} S_n>\lambda \}$ into the events $A_k:=\{ \abs{S_k}>\lambda\  \text{and} \ \abs{S_j}\le \lambda \text{\ for all $j<k$} \}$, then we have
\begin{equation*}
  \begin{aligned}
    \mathbb{E}[S_n^2]\ge \mathbb{E}[S_n^2\,\mathbf{1}_{A^*}] & =\sum^n_{k=1}\mathbb{E}[S_n^2\,\mathbf{1}_{A_k}]\\ 
&=\sum^n_{k=1}\left( \mathbb{E}[S^2_k\,\mathbf{1}_{A_k}]+2\mathbb{E}[S_k(S_n-S_k)\,\mathbf{1}_{A_k}] +\mathbb{E}[(S_n-S_k)^2\,\mathbf{1}_{A_k}]\right)\\ 
&\ge \sum^n_{k=1}\mathbb{E}[S_k^2\,\mathbf{1}_{A_k}]\ge \sum^n_{k=1}\lambda^2\mathbb{P}(A_k)=\lambda^2 \mathbb{P}(A^*).
  \end{aligned}
\end{equation*}

% Moreover, let $X_1, X_2,\cdots$ be independent, mean-zero and $\mathbb{E}[X_k^2] <\infty\ \forall k\in \mathbb{N}_+$. If $$\sum^\infty_{n=1}\mathbb{E}[X_n^2]<\infty,$$ then $$\sum^\infty_{n=1}X_n<\infty\ a.s..$$ In particular, if 
% \[
% \sum^\infty_{k=1} \frac{\mathbb{E}[X_k^2]}{k^2}<\infty.
% \]
% then $$\sum^\infty_{n=1}\frac{X_n}{n}<\infty\ a.s..$$

Second, let $X_1, X_2,\cdots$ be independent, mean-zero and $\mathbb{E}[X_k^2] <\infty\ \forall k\in \mathbb{N}_+$, then 
\[
\sum^\infty_{i=1}\mathbf{Var}(X_i)<\infty \Rightarrow \sum^\infty_{i=1}X_i <\infty\ a.s..
\]
By the assumptions about $\{X_i\}_{i\ge 1}$, we see $\{S_n\}_{n\ge 1}$ is a the Cauthy sequence in $L^2(\Omega)$ space. Therefore, there exist a $S_\infty\in L^2(\Omega)$ such that $S_n\rightarrow S_\infty$ in $L^2(\Omega)$. Moreover, there exist a subsequence $\{n_k\}_{k\ge 1}$ such that $S_{n_k}\rightarrow S_\infty\ a.s..$.

For any $k\ge 0$ (let $n_0:=0, S_0=0$), by Kolmogorov inequality, we have
\[
\mathbb{P}\left( \max_{n_k<p\le n_{k+1}}\abs{S_p-S_{n_k}}\ge \varepsilon \right)\le \frac{1}{\varepsilon^2}\mathbb{E}[\abs{S_{n_{k+1}}-S_{n_k}}^2].
\]
Note that 
\[
\sum^\infty_{k=1}\mathbb{E}[\abs{S_{n_{k+1}}-S_{n_k}}^2]=\sum^\infty_{n=1}\mathbb{E}[X_n^2]<\infty.
\]
By Borel-Cantelli lemma, we obtain that
\[
\max_{n_k<p\le n_{k+1}}\abs{S_p-S_{n_k}}\rightarrow 0\ a.s..
\]
Combining $S_{n_k}\rightarrow S_\infty\ a.s.$, we can obtain $S_n\rightarrow S_\infty\ a.s..$ which called
{\bf Extract Subsequence Method}. In fact, for $n$ large enough, there exists a unique $k$ large enough such that $n_{k-1}<n\le n_k$, then
\[
\abs{S_n-S_\infty}\le \abs{S_{n_{k-1}}-S_\infty}+\max_{n_{k-1}<m\le n_k}\abs{S_m-S_{n_{k-1}}}\rightarrow 0\ a.s..
\]
{\bf Kronecker's lemma} Let $\{ a_n \}_{n\ge 1}$ is a sequence of real number, and suppose $b_n\uparrow \infty$. If $\sum_i 
\frac{a_i}{b_i}<\infty$, then $\frac{\sum^n_{k=1} a_k}{b_n}\rightarrow 0$.


Finally, let $a_n=\mathbf{X}_n(\omega)$ and $b_n=\sqrt{n (\log n)^{1+\varepsilon}}\uparrow \infty$, it suffices to show that $$\sum^\infty_{k=1}\frac{\mathbf{X}_k}{\sqrt{n (\log n)^{1+\varepsilon}}}<\infty \ a.s..$$ We only need to check
\[
\sum^\infty_{i=1} \mathbf{Var}\left(\frac{\mathbf{X}_k}{\sqrt{n (\log n)^{1+\varepsilon}}}\right)=\sum^\infty_{i=1}\frac{\mathbf{Var}(\mathbf{X}_k)}{n (\log n)^{1+\varepsilon}}=\sum^\infty_{i=1}\frac{1}{n (\log n)^{1+\varepsilon}}<\infty.
\]
Let $f(x)=\frac{1}{x(\log x)^\alpha}$, for $x$ large enough, we have $f(x)>0$ and $f$ is a monotonically decreasing continuous function about $x$. Define $F(x)=\log\log x$ if $\alpha=1$, $F(x)=\frac{1}{1-\alpha}(\log x)^{1-\alpha}$ if $\alpha\neq 1$, then we have $F'(x)=f(x)$ for $x$ large enough. Therefore,
\[
\int^\infty_N f(x)\,dx=\begin{cases}
  \infty, \text{\quad \quad \quad\quad \quad \ \ \ if\ } \alpha\ge 1,\\ 
\frac{1}{\alpha-1}(\log N)^{1-\alpha}, \ \ \text{if\ } \alpha>1.
\end{cases}
\]
\end{proof}
\noindent Similarly, we can obtain  that for any $\varepsilon>0$, $k\in \mathbb{N}_+$
 \[ \lim_{n\rightarrow \infty}\frac{\mathbf{S}_n}{\sqrt{n\log n\log^{(2)}n\cdots (\log^{(k)} n)^{1+\varepsilon}}}=0\ a.s..
\]
The best possible rate was obtained by Khinchine which is called Law of  Iterated Logarithm,
\[
\limsup_{n\rightarrow \infty}  \frac{S_n}{{\sqrt{2n\log\log n}}}=1\ a.s..
\]


\begin{lemma}\label{lm3}
For any positive integer $N$, we have 
\[
\mathbb{P}\left(S_n\ge k\right)\le e^{-\frac{k^2}{2n}}
\]
\end{lemma}
\begin{proof}
  Since $$\mathbb{P}\left(S_n\ge k\right)\le \frac{\mathbb{E}[e^{tS_n}]}{e^{tk}}=\frac{\left(\mathbb{E}[e^{tX_1}]\right)^n}{e^{tk}}$$
and 
\[
\mathbb{E}[e^{tX_1}]=\frac{e^t+e^{-t}}{2}\le e^{\frac{t^2}{2}}.
\]
By taking $t=\frac{k}{n}$, we have
\[
\mathbb{P}\left(S_n\ge k\right)\le \frac{e^{\frac{nt^2}{2}}}{e^{tk}}=e^{-\frac{k^2}{2n}}.
\]
\end{proof}
\begin{lemma}[Reflection principle]
  For any positive integer $m$, we have
\begin{equation*}
  \begin{aligned}
     & {\color{red}\mathbb{P}(M^+_n\ge m, S_n=s)}=\begin{cases}
\mathbb{P}(S_n=s), \text{ if $s\ge m$},\\
{\color{red}\mathbb{P}(S_n=2m-s), \text{ if $s< m$}},
\end{cases}
  \end{aligned}
\end{equation*}
and 
\begin{equation*}
  \begin{aligned}
     & \mathbb{P}(M_n^+\ge m)=\mathbb{P}(S_n\ge m) +\sum^{m-1}_{s=-\infty} \mathbb{P}(S_n=2m-s)=\mathbb{P}(S_n=m)+\sum^\infty_{k=m+1} 2\mathbb{P}(S_n=k)
  \end{aligned}
\end{equation*}
and thus
\[
{\color{red}\mathbb{P}(M_n^+\ge m)}=2\mathbb{P}(S_n\ge m+1)+\mathbb{P}(S_n=m){\color{red}\le 2\mathbb{P}(S_n\ge m)}.
\]
\end{lemma}

\begin{lemma}\label{2.6}
  $$\mathbb{P}(S_n=k)\sim \frac{e^{-\frac{k^2}{2n}}}{\sqrt{\pi n}}.$$
\end{lemma}
\begin{proof}
Recall Stirling's approximation
\begin{equation*}
  \begin{aligned}
     & n!\sim \sqrt{2\pi n}\left(\frac{n}{e}\right)^n
  \end{aligned}
\end{equation*}
Then, we have
  \begin{equation*}
    \begin{aligned}
      \mathbb{P}\left( S_{2n}=2k \right) &=\frac{(2n)!}{(n+k)!(n-k)!} 2^{-2n}\\
&\sim \frac{1}{\sqrt{\pi n}} \frac{1}{\left( 1+\frac{k}{n} \right)^{n+k}\left( 1-\frac{k}{n} \right)^{n-k}} \frac{1}{\sqrt{\left(1+\frac{k}{n} \right)\left(1-\frac{k}{n} \right)}}\\ 
&=\frac{1}{\sqrt{\pi n}} \frac{\left( 1-\frac{k}{n} \right)^k}{\left( 1-\frac{k^2}{n^2} \right)^{n+\frac{1}{2}} \left( 1+\frac{k}{n}\right)^k}
    \end{aligned}
  \end{equation*}
Note that we need $(n-k)\rightarrow \infty$ to use Stirling's formula. We choose $k=\floor{x\sqrt{\frac{n}{2}}}$ so that $\frac{2k}{\sqrt{2n}}\rightarrow x$.
It is not hard to see that if $x_n\rightarrow 0$, and $y_n\rightarrow \infty$ such that $x_n y_n\rightarrow t$, then $(1+x_n)^{y_n}\rightarrow e^t$. Therefore, 
\begin{equation*}
  \begin{aligned}
     & \mathbb{P}(S_{2n}=2k)\sim \frac{1}{\sqrt{\pi n}} \left( 1-\frac{x^2}{2n} \right)^{-n}\left(1-\frac{x}{\sqrt{2n}} \right)^{x\frac{\sqrt{n}}{\sqrt{2}}} \left(1+\frac{x}{\sqrt{2n}}  \right)^{-x\frac{\sqrt{n}}{\sqrt{2}}}\rightarrow \frac{1}{\sqrt{\pi n}} e^{-\frac{x^2}{2}}.
  \end{aligned}
\end{equation*}



\end{proof}
\begin{lemma}
  Let $k>n^\frac{1}{2}$, then there exist a constant $C$ such that 
\[
\mathbb{P}(S_n\ge k)\ge C\cdot \frac{n^\frac{1}{2}}{k}e^{-\frac{k^2}{2n}}.
\]
\end{lemma}
\begin{proof} It is easy to see
  \[
\mathbb{P}(S_n\ge k)\ge \mathbb{P}(k\le S_n\le k+\frac{n}{k})\ge C\cdot n^{-\frac{1}{2}}\sum^{k+\frac{n}{k}}_{m=k} e^{-\frac{m^2}{2n}}.
\]
For $k\le m\le k+\frac{n}{k}$, we have
\[
e^{-\frac{k^2}{2n}}\ge e^{-\frac{m^2}{2n}}\ge e^{-\frac{(k+\frac{n}{k})^2}{2n}}=\exp\left( -\frac{k^2}{2n}-1-\frac{n}{2k^2} \right)\ge C\cdot e^{-\frac{k^2}{2n}}.
\]
Therefore, we have
\[
\mathbb{P}(S_n\ge k)\ge C\cdot \frac{n^\frac{1}{2}}{k}e^{-\frac{k^2}{2n}}.
\]







\end{proof}
\begin{theorem}
  Define $F_n:={\sqrt{2n\log\log n}}$, then we have
\[
\limsup_{n\rightarrow \infty}  \frac{S_n}{F_n}=1\ a.s..
\]
\end{theorem}
\begin{proof}
  The proof will be presented in two steps. The first one gives an upper bound of $\limsup_{n\rightarrow \infty} \frac{S_n}{F_n}$, i.e. for any $\varepsilon>0$, we show that $$\limsup_{n\rightarrow \infty} \frac{S_n}{F_n}\le 1+\varepsilon\ a.s..$$
The second one gives a lower bound of $\limsup_{n\rightarrow \infty}\frac{S_n}{F_n}$, i.e. for $0<\varepsilon<\frac{1}{2}$, $$\limsup_{n\rightarrow \infty}\frac{S_n}{F_n}\ge 1-\varepsilon\ a.s..$$




{\bf Step 1.} Let $\Theta>1$, $n_k:=\floor{\Theta^k}$, by reflection principle and Lemma \ref{lm3}, we have
\begin{equation*}
  \begin{aligned}
    \mathbb{P}\left( M^+_{n_k} \ge (1+\varepsilon)F_{n_k}\right)  &\le 2\mathbb{P}\left( S_{n_k}\ge   (1+\varepsilon)F_{n_k}\right)\\ 
&\le 2\exp\left( -\frac{(1+\varepsilon)^2F^2_{n_k}}{2n_k} \right)\\ 
&=2\exp\left( -(1+\varepsilon)^2\log\log n_k \right)\sim(k\log \Theta)^{-(1+\varepsilon)^2}
  \end{aligned}
\end{equation*}
By Borel-Cantelli lemma, we have
\[
M^+_{n_k}\le (1+\varepsilon)F_{n_k} \ a.s.,
\]
for all but finitely many $k$.

Let $n_k\le n<n_{k+1}$,  
\[
\frac{S_n}{F_n}\le\frac{M^+_n}{F_n}=\frac{M^+_{n_{k+1}}}{F_{n_{k+1}}}\frac{F_{n_{k+1}}}{F_{n}}\frac{M^+_n}{M^+_{n_{k+1}}}\le (1+\varepsilon)\frac{F_{n_{k+1}}}{F_{n}} \le 1+2\varepsilon\ a.s.,
\]
where $\Theta(\varepsilon,k)$ is close enough to $1$.

{\bf Step 2.} Define $\mathbf{n}_k=n_{k+1}-n_k$, by the definition of $\{S_n\}_{n\ge 1}$, we have $\{S_{n_{k+1}}-S_{n_k}\}_{k\ge 1}$ is mutually independent, $S_{\mathbf{n}_k}$ and $S_{n_{k+1}}-S_{n_k}$ are equal in law. By Lemma \ref{2.6}, for large $k(\varepsilon)$ enough, we have
% \begin{equation*}
%   \begin{aligned}
%      \mathbb{P}\left(S_{\mathbf{n}_k}=S_{n_{k+1}}- S_{n_{k}}\ge (1-\varepsilon)F_{{n}_{k+1}}\right)
%   \end{aligned}
% \end{equation*}
\begin{equation*}
  \begin{aligned}
      \mathbb{P}\left(S_{\mathbf{n}_k}=S_{n_{k+1}}- S_{n_{k}}\ge (1-\varepsilon)F_{\mathbf{n}_k}\right)&\ge C\cdot \frac{\mathbf{n}_k^\frac{1}{2}}{(1-\varepsilon)F_{\mathbf{n}_k}}\exp\left( -\frac{(1-\varepsilon)^2 F^2_{\mathbf{n}_k}}{2\mathbf{n}_k} \right)\\
&{\color{red}\sim \frac{1}{\sqrt{\log \log \mathbf{n}_k}}(\log \mathbf{n}_k)^{-(1-\varepsilon)^2}}
  \end{aligned}
\end{equation*}
where the last sim is due to $0<\varepsilon<\frac{1}{2}$.  Note that $\log \mathbf{n}_k\sim k$, we have $\sum^\infty_{k=1}\mathbb{P}(S_{\mathbf{n}_k}\ge(1-\varepsilon)F_{\mathbf{n}_k})=\infty$, by Borel-Cantelli lemma, we have 
\[
S_{n_{k+1}}\ge S_{n_k}+(1-\varepsilon) F_{\mathbf{n}_k} \quad i.o. \ a.s..
\]
By the symmetric of the upper bound, we have
\[
\liminf_{k\rightarrow \infty} \frac{S_{n_k}}{F_{n_k}}\ge \liminf_{n\rightarrow \infty} \frac{S_{n}}{F_{n}}\ge -(1+\varepsilon)\ a.s..
\]
Therefore, we have
\begin{equation*}
  \begin{aligned}
     \frac{S_{n_{k+1}}}{F_{n_{k+1}}} &\ge \frac{S_{n_k}}{F_{n_k}}\frac{F_{n_k}}{F_{n_{k+1}}}+(1-\varepsilon)\frac{F_{\mathbf{n}_k}}{F_{n_{k+1}}}\\ 
&\ge -(1+\varepsilon) \frac{F_{n_k}}{F_{n_{k+1}}}+(1-\varepsilon)\frac{F_{\mathbf{n}_k}}{F_{n_{k+1}}}\rightarrow -\frac{(1+\varepsilon)}{\Theta^\frac{1}{2}}+(1-\varepsilon)\left( \frac{\Theta-1}{\Theta} \right)^\frac{1}{2}.
  \end{aligned}
\end{equation*}
Take $\varepsilon \rightarrow 0^+$ and $\Theta \rightarrow \infty$, we complete the proof.
\end{proof}
Using almost the same method, we can prove the result about Brownian motion. For the convenience of the readers, we provide the detailed proof.
\begin{theorem}
  For a Brownian motion $B$ in $\mathbb{R}$, we have
\[
\limsup_{t\rightarrow \infty}\frac{B_t}{\sqrt{2t\log\log t}}=1\ a.s..
\]
\end{theorem}
\begin{proof}
when $u\rightarrow \infty$, we have
\[
\int_u^{\infty} e^{-\frac{1}{2}}\,dx\sim u^{-1}\int^\infty_u x e^{-\frac{x^2}{2}}=u^{-1}e^{-\frac{u^2}{2}}.
\]
In fact,
\[
\int_u^\infty \frac{1}{x} (xe^{-\frac{x^2}{2}})\,dx=\int^\infty_u \frac{1}{x}d(e^{-\frac{x^2}{2}})=\left.\frac{1}{x} e^{-\frac{x^2}{2}}\right|^\infty_0-\int^\infty_u \frac{1}{x^2} e^{-\frac{x^2}{2}}\,dx.
\]
Let $M^+_t=\sup_{0\le s\le t} B_s$, then by reflection principle, we have 
\[
\mathbb{P}(M_t^+> ut^\frac{1}{2})=2\mathbb{P}(B_t>u t^\frac{1}{2})\sim \frac{2}{\sqrt{2\pi}} u^{-1} e^{-\frac{u^2}{2}}.
\]

{\bf Step 1.}\ Define $F_t=\sqrt{2t\log\log t}$, for any $\Theta>1$ and $1+\varepsilon>1$, for $n$ large enough, we have
\begin{equation*}
  \begin{aligned}
   &\mathbb{P}(M^+_{\Theta^n}> (1+\varepsilon)F_{\Theta^{n}}) \\
\le& 2\mathbb{P}\left(\frac{B_{\Theta^n}}{\sqrt{\Theta^{n}}}> \frac{(1+\varepsilon) F_{\Theta^{n}}}{\sqrt{\Theta^n}} \right)\\ 
\lesssim& \sqrt{\frac{\Theta^n}{(1+\varepsilon)^2F^2_{\Theta^{n}}}}\exp\left( -\frac{1}{2} \frac{(1+\varepsilon)^2 F^2_{\Theta^{n}}}{\Theta^n} \right)\\ 
\lesssim& \frac{1}{\sqrt{\log \log \Theta^n}} \exp\left( -(1+\varepsilon)^2\log\log \Theta^n \right)\sim(n\log \Theta)^{-(1+\varepsilon)^2}.
  \end{aligned}
\end{equation*}
By the Borel-Cantelli lemma, we obtain that for $n$ large enough,
\[
\frac{M^+_{\Theta^n}}{F_{\Theta^n}}\le (1+\varepsilon)\ a.s..
\]
Therefore, for $\Theta^n\le t<\Theta^{n+1}$, $\Theta$ approach $1$,
\[
\frac{B_t}{F_t}\le \frac{M^+_{t}}{F_t}=\frac{M^+_{\Theta^{n+1}}}{F_{\Theta^{n+1}}}\frac{F_{\Theta^{n+1}}}{F_{t}}\frac{M^+_t}{M^+_{\Theta^{n+1}}}\le (1+\varepsilon)\frac{F_{\Theta^{n+1}}}{F_{t}} \le 1+2\varepsilon\ a.s..
\]




{\bf Step 2.}\ For $0<\varepsilon<\frac{1}{2}$, we have
\begin{equation*}
  \begin{aligned}
    \mathbb{P}\left( B_{\Theta^{n+1}}-B_{\Theta^{n}}> (1-\varepsilon)F_{[\Theta^{n+1}-\Theta^n]}\right) & \ge C\cdot \frac{(\Theta^{n+1}-\Theta^n)^\frac{1}{2}}{F_{[\Theta^{n+1}-\Theta^n]}} \exp\left( -\frac{(1-\varepsilon)^2 F^2_{[\Theta^{n+1}-\Theta^{n}]}}{2(\Theta^{n+1}-\Theta^n)} \right)\\
&\sim \frac{1}{\sqrt{\log \log (\Theta^{n+1}-\Theta^n)}}\left(\log (\Theta^{n+1}-\Theta^n)\right)^{-(1-\varepsilon)^2}
  \end{aligned}
\end{equation*}
Since $\log(\Theta^{n+1}-\Theta^n)\sim n$, we have 
\[
\sum^\infty_{n=1}\mathbb{P}\left( B_{\Theta^{n+1}}-B_{\Theta^{n}}> (1-\varepsilon)F_{[\Theta^{n+1}-\Theta^n]}\right)<\infty.
\]
By Borel-Cantelli lemma, we have 
\[
B_{\Theta^{n+1}}\ge B_{\Theta^n}+(1-\varepsilon) F_{[\Theta^{n+1}-\Theta^n]} \quad i.o. \ a.s..
\]
By the symmetric of the upper bound, we have
\[
\liminf_{n\rightarrow \infty} \frac{B_{\Theta^n}}{F_{\Theta^n}}\ge \liminf_{t \rightarrow \infty} \frac{B_{t}}{F_{t}}\ge -(1+\varepsilon)\ a.s..
\]
Therefore, we have
\[
\frac{B_{\Theta^{n+1}}}{\Theta^{n+1}}\ge B_{\Theta^n}+(1-\varepsilon) F_{[\Theta^{n+1}-\Theta^n]} \quad i.o. \ a.s..
\]
\begin{equation*}
  \begin{aligned}
 \frac{B_{\Theta^{n+1}}}{\Theta^{n+1}} &\ge \frac{B_{\Theta^n}}{F_{\Theta^n}}\frac{F_{\Theta^n}}{F_{\Theta^{n+1}}}+(1-\varepsilon)\frac{F_{[\Theta^{n+1}-\Theta^n]}}{F_{\Theta^{n+1}}}\\ 
&\ge -(1+\varepsilon) \frac{F_{\Theta^n}}{F_{\Theta^{n+1}}}+(1-\varepsilon)\frac{F_{[\Theta^{n+1}-\Theta^n]}}{F_{\Theta^{n+1}}}\rightarrow -\frac{(1+\varepsilon)}{\Theta^\frac{1}{2}}+(1-\varepsilon)\left( \frac{\Theta-1}{\Theta} \right)^\frac{1}{2}.
  \end{aligned}
\end{equation*}
Take $\varepsilon \rightarrow 0^+$ and $\Theta \rightarrow \infty$, we complete the proof.
\end{proof}





% \[
% \limsup_{n\rightarrow \infty} b_nM_n\le 1+\varepsilon\ a.s..
% \]
% Recall that  for any $\varepsilon >0$,
% \[
% \mathbb{P}\left( M_n\ge n^\frac{1}{2} x_n \right)\le (1+\varepsilon) \frac{4}{\sqrt{2\pi}} \frac{1}{x_n} e^{-\frac{x_n^2}{2}},
% \]
% where $0<x_n=o(n^\frac{1}{6})$.

% Note that $b_n^{-1}=\sqrt{2n\log\log n}=o(n^\frac{2}{3})$, $\frac{1}{\sqrt{\log\log n}}\rightarrow 0$, we have 
% \begin{equation*}
%   \begin{aligned}
%      & \mathbb{P}\left( M_n\ge (1+\varepsilon)b_n^{-1} \right)\lesssim \exp(-(1+\varepsilon)\log\log n)=(\log n)^{-1-\varepsilon},
%   \end{aligned}
% \end{equation*}
% if $n$ is large enough.

% Let $n_k:=\floor{\Theta^k}$($\Theta>1$). Then by the  Borel-Cantelli Lemma, we obtain
% \[
% M_{n_k}\le (1+\varepsilon)b^{-1}_{n_k}\ a.s.,
% \]
% for all but finitely many $k$. Let $n_k\le n< n_{k+1}$ , then 
% \[
% \frac{M_n}{b_n^{-1}}=\frac{M_{n_{k+1}}}{b^{-1}_{n_{k+1}}}\frac{b^{-1}_{n_{k+1}}}{b^{-1}_{n}}\frac{M_n}{M_{n_{k+1}}}\le (1+\varepsilon)\frac{b^{-1}_{n_{k+1}}}{b^{-1}_{n}} \le 1+2\varepsilon\ a.s.,
% \]
% % \[
% % M_n\le M_{n_{k+1}}\le(1+\varepsilon)b^{-1}_{n_{k+1}}\le (1+2\varepsilon)b^{-1}_{n_k}\le(1+2\varepsilon) b_n^{-1}\ a.s.,
% % \]
% where $\Theta(k,\varepsilon)$ is close enough to $1$. We obtain $\limsup_{n\rightarrow \infty} b_nM_n\le 1$.

% {\bf Consider $S_n$:}

% {\bf Step 2.}
% For any $\varepsilon >0$, if $k$ is large enough, then
% \begin{equation*}
%   \begin{aligned}
%      & \mathbb{P}\left( b_{n_{k+1}}(S_{n_{k+1}}-S_{n_k}) \ge 1-\varepsilon\right)\ge O\left(\frac{1}{(\log n_{k+1})^{1-\frac{\varepsilon}{2}}}\right)
%   \end{aligned}
% \end{equation*}






% \section{Acknowledgments}

% I would like to thank Professor Xue-Mei Li. I am indebted to her for her help when I study at Imperial College London as a visiting student.
\bibliographystyle{phd}

\newpage
%-----------------------?
\noindent

%\bibliographystyle{plain}
% \bibliography{ref}

% \nocite{*}

\end{document}



